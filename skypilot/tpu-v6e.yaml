name: maxtext-tpu-train

resources:
  cloud: kubernetes

# 同步本地代码到远程
file_mounts:
  /workspace/maxtext:
    source: .
    mode: COPY

# 环境变量
envs:
  OUTPUT_PATH: gs://your-bucket/output
  DATASET_PATH: gs://your-bucket/dataset
  RUN_NAME: maxtext-tpu-v6e-run

# 节点数量（对应 TPU slice 中的 worker 数量）
# 4x8 topology = 32 chips, 每个 pod 4 chips = 8 个 worker
num_nodes: 8

setup: |
  cd /workspace/maxtext

  # 安装 MaxText
  pip install -e . --quiet

  # TPU 特定依赖（如果镜像中没有预装）
  pip install -r dependencies/requirements/generated_requirements/tpu-requirements.txt --quiet || true

  # 验证 JAX 和 TPU
  python3 -c "
  import jax
  print(f'JAX version: {jax.__version__}')
  print(f'Devices: {jax.devices()}')
  print(f'Device count: {jax.device_count()}')
  "

run: |
  cd /workspace/maxtext

  # 多主机 TPU 训练
  python3 -m MaxText.train \
    src/MaxText/configs/base.yml \
    model_name='llama2-7b' \
    base_output_directory=${OUTPUT_PATH} \
    dataset_path=${DATASET_PATH} \
    run_name=${RUN_NAME} \
    per_device_batch_size=4.0 \
    steps=100 \
    enable_checkpointing=true \
    checkpoint_period=500 \
    log_period=50
