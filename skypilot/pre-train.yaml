name: maxtext-tpu-train

resources:
  cloud: kubernetes

num_nodes: 8

file_mounts:
  /workspace/ant-pretrain:
    name: ant-pretrain-code
    source: /Users/hongmao/workplace/ant-pretrain
    mode: COPY

envs:
  OUTPUT_PATH: gs://tpu-service-473302-maxtext-output/
  DATASET_PATH: gs://tpu-service-473302-maxtext-dataset/
  RUN_NAME: maxtext-tpu-v6e-run

setup: |
  cd /workspace/ant-pretrain

  conda create --name ant-pretrain -c conda-forge python=3.12 -y
  conda activate ant-pretrain

  bash tools/setup/setup.sh DEVICE=tpu

run: |
  cd /workspace/ant-pretrain
  conda activate ant-pretrain

  echo "=== All Environment Variables ==="
  env | grep -E "(SKYPILOT|RAY|TPU|JAX)" | sort
  echo "================================="
  export TPU_WORKER_HOSTNAMES=$(echo "${SKYPILOT_NODE_IPS}" | tr ' ' ',' | tr '\n' ',' | sed 's/,$//')
  export TPU_WORKER_ID=$SKYPILOT_NODE_RANK

  echo "=== After Setting ==="
  echo "TPU_WORKER_ID: ${TPU_WORKER_ID}"
  echo "TPU_WORKER_HOSTNAMES: ${TPU_WORKER_HOSTNAMES}"
  echo "====================="

  python3 -m src.MaxText.train \
    src/MaxText/configs/base.yml \
    model_name='llama2-7b' \
    base_output_directory=${OUTPUT_PATH} \
    dataset_path=${DATASET_PATH} \
    run_name=${RUN_NAME} \
    per_device_batch_size=4.0 \
    steps=100 \
    enable_checkpointing=true \
    checkpoint_period=500 \
    log_period=50
